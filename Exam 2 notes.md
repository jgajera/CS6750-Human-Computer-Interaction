# **EXAM #2 NOTES**



**3.5 Prototyping**

**[GOAL #1] Students will understand the role of prototyping in the design life cycle.**



*   Like brainstorming design alternatives, **prototyping** involves looking at the different ideas available to us and developing them a bit.
    *   The major distinction, though, is that in prototyping, we want to actually build things we can put in front of users.
    *   But that doesn’t mean building the entire interface before we ever have a user look at it --- We want to get user feedback as quickly and rapidly as possible, and build up more sophisticated prototypes as we go through the design life cycle.
*   **Timing**
    *   **Early prototyping**: very rapid revision on preliminary ideas. This happens on our first few iterations through the design life cycle.
    *   **Late prototyping**: finishing touches on a final design, or revising a design that’s already live. This happens after we’ve already been through several iterations of our design life cycle.

**[GOAL #2] Students will understand the spectrum from low- to high-fidelity prototypes and which are appropriate at the right times.**



*   **Representations**
    *   Text
    *   Paper
    *   Wireframe
    *   Physical
    *   Live

   
*   **Fidelity**: refers to the completeness or maturity of the prototype.
    *   **Low fidelity**: something like paper or simple drawings, very easy to change
    *   **High fidelity**: wireframe or a working interface, something harder to put together
    *   We want to use easily changeable low-fidelity prototypes to explore our ideas before moving on to high-fidelity prototypes.
    *   **Trade-offs**
        *   Prototyping is largely about the trade-offs we have to make. Our goal is to maximize these trade-offs.
        *   We want to start with the easier low-fidelity prototypes to get initial ideas, to evaluate big designs and big plans, and to make sure we’re on the right track.
        *   Then as we go along, we can move toward the higher-fidelity prototypes that take more time to assemble because we have some initial evidence that our designs are sound.
        *   It’s really important here to also stress that our prototypes are prototypes. They aren’t complete interfaces.
        *   Don’t become a runaway train of designing: design deliberately and get feedback often

  

*   **Evaluation**
    *   These different kinds of prototypes also lend themselves to different kinds of evaluation structures.
    *   **LF**: fine for evaluating the relative function of an interface, whether or not it can do what it’s designed to do. 
    *   **HF**: can be useful in evaluating the readability of the interface. To evaluate actual performance, like how long certain tasks take or what designs lead to more purchases, we generally need a high-fidelity prototype later in the cycle

    

*   **Scope**
    *   **Vertical prototypes**: take a small portion of the interaction and prototype it in great detail.
    *   **Horizontal prototypes**: cover the design as a whole, but in a more shallow way.
    *   Usually start with the horizontal prototype earlier on and move toward the deeper vertical prototype later, but in reality you’ll likely move back and forth between these approaches throughout the timeline.

    
**[GOAL #3] Students will understand the basic methods of performing prototyping.**



*   **Tips for effective prototyping**
    *   1. **Keep prototypes easy to change**. Your goal here is to enable rapid revision and improvement. It’s easy to make quick changes to something on paper, but it’s harder to make it to code or physical prototypes.
    *   2. **Make it clear that it’s a prototype**. If you make a prototype look too good, users may focus on the superficial elements like colors or font. By letting your prototype look like a prototype, you can help them focus on what you’re ready to test.
    *   3. **Be creative**. Your goal is to get feedback. Do whatever it takes to get feedback. Don’t let the type of prototype you’re designing constrain the kind of feedback you can get: if you find your current prototypes don’t give the right kind of feedback, find ones that do!
    *   4. **Evaluate risks**. One of the biggest goals of prototyping is to minimize the time spent pursuing bad designs by getting feedback early. Ask yourself consistently: how much would I lose if I found users hate the parts of my design they haven’t seen yet? Whenever that answer gets larger than a couple hours, try to get feedback to make sure you’re not wasting time.
    *   5. **Prototype for Feedback**. The goal of a prototype is to get feedback. You could spend a lot of time focusing on things like font selection and color choice, but that’s probably not the feedback you need when you’re exploring your big alternatives. Prototype for the kind of feedback you want to get.
*   **Verbal prototypes**
    *   Very simplest of prototypes; lowest-fidelity; literally just describing the design we have in mind.
    *   it’s extremely easy to do, although it can be hard to do effectively.
    *   _Social desirability bias_ is big here because it’s difficult to describe our idea in a way that allows the participant to feel comfortable agreeing with us. So, we need to make sure to ask for specific and critical feedback.
    *   At the same time, though, how do we really know the user understands the design we’re describing? We’re working toward becoming experts in these areas, and we don’t want to fall victim to _expert blindspot_ by assuming our explanation makes sense to a novice. _Analogies_ can be powerful tools for explaining prototypes: describe your interface in terms of other tools the user might know about.
*   **Paper prototypes**
    *   One step above just ‘describing our ideas’ to our user in a verbal prototype would be drawing them out. This is what we call a paper prototype (could do this for anything from designing an on-screen interface to designing the placement of controls in a vehicle to designing a physical interaction device)
    *   Paper prototyping isn’t only useful for testing out big interface design. You can also do some interaction with it (also called _card-based prototyping_ -- the idea is each screen would be on a card that I can quickly sub in and out)
    *   Allows us to prototype a decent amount of my interface’s interaction with relatively low prototyping effort
*   **Wizard of Oz prototypes**
    *   Paper prototyping is great when we’re designing flat interfaces for screens, but WoO is great if you were designing a voice interface or a gesture interface
    *   The idea here is that we, “behind the curtain”, do the things that the interface would do once it’s implemented ⇒ that way, we can test out the interactions that we plan to design and see how well they’ll work.
    *   In practice, Wizard of Oz prototypes can actually get very complex. You could have entire programs that work by having a person supply the requested input at the right time.
    *   As a concept, though, a Wizard of Oz prototype is a prototype where the user can interact with the system somewhat authentically, while a human supplies the functionality that hasn’t yet been implemented.
*   **Wireframing prototypes**
    *   In wireframing, we use some more detailed tools to mock up what an interface might look like. This lets us experiment with some additional details like font size, colors, and the challenges of screen real estate.
    *   There are lots of tools out there for wireframing that come equipped with built-in common widgets and layouts, but you can also do some rudimentary wireframing in something as simple as PowerPoint or Google Drawings.
*   **Physical prototypes**
    *   Wireframing is great for prototyping on-screen interfaces, but physical prototypes are if you’re working on something more physical and three-dimensional
    *   It doesn’t have to actually work; that’s where a lot of designers get tripped up: they think to get good feedback on a design, they have to have a working version, but you don’t. There are lots of elements you can test without actually implementing anything
*   **Prototyping pros and cons**


*   **Design lifecycle**
    *   We did some initial needfinding, brainstormed some alternatives, and prototyped those alternatives on paper.
    *   Now, we evaluate those prototypes with real people.
    *   The results of that evaluation tell us if we need to go back and understand the task better.
    *   That evaluation tells us the new questions we need to ask about the users’ tasks.
    *   Those results help us reflect on our alternatives as a whole.
    *   Then, equipped with that initial evaluation, the additional needfinding, and the additional brainstorming, we return here and prototype again.
    *   If our prototype was pretty successful, we might increase the fidelity, increase the detail, and evaluate again.
    *   If it wasn’t, we might stay low-fidelity with a different design altogether.
    *   Each time we develop a new prototype, we’re going through this cycle again

---


**2.7 Task Analysis**

**[GOAL #1] Students will understand the nature of task analysis in formalizing our understanding of user tasks.**



*   **Human information processor models** (includes the GOMS model): focuses on the input and output.
    *   Notice this is similar to the processor model of the user we discuss elsewhere.
*   **Cognitive task analysis**: a way of trying to get inside the user’s head instead of focusing on input and output
    *   Notice that this is similar to the predictor model of the user.

**[GOAL #2] Students will understand GOMS and cognitive task analysis as two methods for performing task analysis.**



*   **GOMS**
    *   It builds off of the processor model of the human’s role in a system.
    *   GOMS Model gets its name from the four sets of information it proposes gathering about a task:
        *   Goals: the user’s goals in the system.
        *   Operators: the operators the user can perform in the system.
        *   Methods: the methods for achieving goals in the system.
        *   Selection rules: the rules for choosing between competing methods.
*   The GOMS Model thus proposes that every human interacting with a system has a set of goals they want to accomplish.
    *   To accomplish those goals, they have methods, which are composed of series of operators.
    *   They also have rules for choosing between multiple methods that would each get them to the goal.
*   **Strengths and weaknesses of GOMS**
    *   One weakness is that it does not automatically address a lot of the complexity of problems.
    *   A second weakness is that the GOMS model assumes the user already has methods in mind (assumes the user is an expert).
        *   That means the user is already an expert in the area.
        *   GOMS does not do a good job of accounting for novices or accounting for user errors.
    *   The strength of GOMS, on the other hand, is its ability to formalize user interaction into steps that we can actually use to make predictions.
        *   We can measure how long it takes to move between each step, and so we can predict the overall efficiency of using a particular interface. And that can focus our attention on where things can be improved.
*   **Tips for developing GOMS models**
    *   1. **Focus on small goals**. We’ve used some pretty big examples, but GOMS was really designed to work in the context of very small goals, like navigating to the end of a document. You can abstract up from there, but start by identifying smaller, moment-to-moment goals.
    *   2. **Nest goals, not operators**. It’s possible to nest goals. For example, in our GOMS model of navigation, we could develop it further and break the overall task of navigating to a destination down to smaller goals like changing lanes or plotting routes. Operators, however, are the smallest “atom” of a GOMS model. These don’t break down any further, and these must be actions that are performed.
    *   3. **Differentiate descriptive and prescriptive**. Make sure to identify whether you’re building a model of what people do, or what you want them to do. You can build a GOMS model of what users should do with your interface, but you shouldn’t trick yourself into thinking that’s what they will do.
    *   4. **Assign costs to operators**. GOMS was designed to let us make predictions about how long certain methods will take. The only way we can do that is if we have some measurement of how long the individual operations take. Usually this is time, but depending on the domain, we might be interested in phrasing the cost differently as well.
    *   5. **Use GOMS to trim waste**. One of the benefits of GOMS is it helps you visualize where an unnecessary number of operators may be required to accomplish a task. This is bolstered by the costs assigned to them. Use GOMS to identify places where the operators required can be simplified by the interface.
*   **Variations of GOMS**
    *   These variations share the commonality of goals, operators, methods, and selection criteria, but they differ in what additional elements they provide.
    *   Bonnie John and David Kieras cover four popular variations in a paper from 1996:
        *   They start with the keystroke-level model, the simplest technique.
            *   Here, the designer simply specifies the operators and execution times for a an action, and sums them to find the complexity of an interaction.
            *   This method proposed six categories of operators: for modern modeling, we would need some new ones to cover touchscreens and other novel interfaces.
        *   CMN-GOMS is an extension of GOMS that features submethods and conditions in a strict goal hierarchy.
        *   Natural GOMS Language, the third, is a natural-language form of GOMS, that lends itself to human interpretation.
            *   In all these cases, the important point of emphasis is the way these models allow us to focus in on places where we might be asking too much of the user.
        *   4th type
            *   For example, in this model, the user was being asked to recall lots of information from memory. By making the assumptions and actions this explicit, this model lets us target where working memory is being overly taxed in a way that we might miss with more high-level designs.
*   **GOMS to cognitive task analysis //// participant+processor+predictor**
    *   The GOMS model is a human information processor model: it largely considers the human to be an input/output machine, and it doesn’t get too far into the internal reasoning of the human. Instead, it stills them into things that can be described explicitly, like goals and methods.
    *   Some would argue, though, that human reasoning is too nuanced and complex to be so simplified. They advocate other models that get more into what goes on inside the user’s head.
    *   That’s where cognitive task analysis comes in: cognitive task analysis is another way of examining tasks, but it puts a much higher emphasis on things like memory, attention, and cognitive load. Thus, cognitive task analysis adopts more of the predictor view of the human’s role in the system.
    *   This conflict between more processor-oriented and more predictor-oriented models of the user actually gets at the core of an old battle in psychology between behaviorism and cognitivism.
        *   **Behaviorism**: an approach to psychology that emphasizes behavior as a product of stimuli and the environment. Behaviorism emphasizes things that can be observed: we can see what input a person is receiving, and we can see the output they are producing, and that may be all we need to understand to design things.
        *   **Cognitivism**: an approach to psychology that emphasizes internal thought processes. Cognitivism, on the other hand, suggests we can and should get into what people are actually thinking inside their minds, how systems like memory and learning and perception work.
        *   Joyner prefers methods that focus on cognition.
        *   Both approaches have significant value, however: The GOMS model and its emphasis on identifying goals and operators is actually very useful in HCI because it forces us to very clearly and deliberately identify user goals and the sequences of actions that accomplish them. We can get so caught up in user experiences that we forget the user experience is born out of the individual operators. So while I wouldn’t advocate focusing solely on the user as an input-output information processor, there is value in defining the user’s operations as clearly and specifically as we define the computer’s.
*   **Cognitive task analysis**
    *   Cognitive Task Analysis is not a single method, but rather a general type of method to approach the evaluation of how people complete tasks. Performing a cognitive task analysis involves a number of techniques and methods that we discuss more when discussing the design life cycle.
    *   For right now, though, we’re interested in what kind of information we’re trying to gather, not how we’re gathering it.
    *   Cognitive task analyses are especially concerned with understanding the underlying thought process in performing a task. Not just what we can see, but specifically what we can’t see.
    *   There are a lot of different methods for performing cognitive task analyses, but most methods follow a particular common sequence:

            

        *   1. **Collect preliminary knowledge**: while we as interface designers don’t need to become experts in a field, we need a good bit of familiarity with it. So, we might observe people performing the task.
        *   2. **Identify knowledge representations**: in other words, what kinds of things does the user need to know to complete their task? Note that we’re not yet concerned with the actual knowledge they have, only the types or structures of the knowledge they have.
            *   For example, we want to know: does this task involve a series of steps in a certain order? A collection of tasks to check off in any order? A web of knowledge to memorize?
        *   3. **Apply focused knowledge elicitation methods**: populate those knowledge representations.
            *   It is in this stage that we start to recognize what the user actually knows.
            *   During this stage, we identify all the specific actions they take, the knowledge they must have in mind to take those actions, the interruptions that can change their thought process, the equipment involved, and the sensory experience of the user.
        *   4. **Analyze and Verify Data Acquired**: part of that is just confirming with the people we observed that our understanding is correct.
            *   Then, we attempt to formalize it into structures that can be compared and summarized across multiple data-gathering methods.
        *   5. **Format Results for the Intended Application**: finally, we need to take those results and format them in a way that is useful for interface design.
            *   We want to develop models that show what the user was thinking, feeling, and remembering at any given time and make those relationships explicit.
    *   **Pros and cons**
        *   **Strength**: Emphasizes mental processes
            *   Unlike the GOMS model, cognitive task analysis puts an emphasis on what goes on inside the user’s head. It is thus much better-equipped to understand how experts think and work.
        *   **Strength**: Formal enough for interface design
            *   The information it generates are formal enough to be used for interface design, for comparison among alternatives, and more.
        *   **Weakness**: Time-intensive
            *   Cognitive task analysis is incredibly time-intensive to perform: it involves talking to multiple experts for extended periods of time, then systematically analyzing their data.
        *   **Weakness**: May deemphasize context
            *   And in zooming in on the individual’s own thought processes, it risks deemphasizing details that are out in the world, like the role of physical capabilities or interactions with others.
        *   **Weakness**: Ill-suited for novices
            *   But perhaps most importantly, cognitive task analysis, like the GOMS model, is well-suited for expert users, but ill-suited for novices.
*   **Hierarchical task analysis**
    *   Cognitive task analysis advocates building models of human reasoning and decision-making in complex tasks. However, a challenge presented here is that very often, large tasks are actually composed of multiple smaller tasks.
    *   You generally will find tasks and subtasks whenever you’re looking at the results of cognitive task analyses.
    *   It’s important to remember the strengths supplied by this hierarchy: abstracting out unnecessary details, modularizing designs or principles, and organizing task analysis.
    *   When you’re creating real cognitive models, you’ll likely have several levels, several states, and additional annotating information.
*   **Other task analysis frameworks: **GOMS and cognitive task analysis are just two of many alternatives to approaching trying to understand how users approach tasks.

    
    *   More in line with GOMS, options exist like KLM, MHP, and TLM, the keystroke-level model or touch-level model, which captures even finer-grained actions for estimating performance speed.
    *   Other extensions to GOMS add subgoals, like CPM-GOMS and NGOMSL.
    *   Cognitive Perceptual Motor-GOMS focuses on parallel tasks, while NGOMSL focuses on creating a natural language.
    *   More along the lines of Cognitive Task Analysis, multiple more specific methods exist as well, including: ACTA, CDM, Skill-Based CTA, TKS, and CFM.
    *   CDM puts a focus on places where critical decisions occur, TKS focuses on the nature of users’ knowledge, and CFM focuses on complexity.
    *   ACTA and Skill-Based CTA are two ways of gathering the information necessary to do a cognitive task analysis



---


**2.8 Distributed Cognition**

**[GOAL #1] Students will understand methods for analyzing systems as having knowledge distributed across multiple artifacts and individuals.**



*   Cognition on its own is interested in the kinds of thought processes and experiences. We naturally think of those as occurring inside the mind. But distributed cognition suggests that models of the mind should be extended outside the mind. This theory proposes expanding the unit we use to analyze intelligence from a single mind to the mind equipped with other minds and artifacts and their relationships.
*   One of the seminal works in distributed cognition research is a paper in the Journal of Cognitive Science from 1995 called “How a Cockpit Remembers its Speeds”.
    *   The cockpit is a collection of controls, sensors, and interfaces, as well as the pilots themselves.
    *   This paper title tells us that it is this entire system: the pilots, the sensors, the controls, and the interfaces amongst them ⇒ that system as a whole is doing the remembering.
    *   No individual part in isolation remembers what the system as a whole remembers.

*   Distributed cognition is deeply related to the idea of cognitive load.
    *   Recall that cognitive load refers to your mind’s ability to only deal with a certain amount of information at a time.
    *   Distributed cognition suggests that artifacts add additional cognitive resources. That means the same cognitive load is distributed across a greater number of resources. Artifacts are like plugging extra memory sticks into your brain.
    *   Distributed cognition suggests models of cognition should be extended outside the mind
*   Every task you offload to artifacts decreases your personal cognitive load (cruise control = artifact; decreasing cognitive load while driving) (paper and pen = artifacts; decreasing cognitive load while doing a complex math problem)
*   Something important to note is that distributed cognition isn’t really another design principle.
    *   Distributed cognition is more of a way of looking at the interface design; it’s a way of approaching the problem that puts your attention squarely on how to extend the mind across artifacts. We could actually view many of our design principles as examples of distributed cognition.
    *   The point is that distributed cognition is a way of looking at interfaces and interface design that focuses your attention on what systems as a whole can accomplish as opposed to individuals on their own.
*   Distributed cognition is a perspective on analyzing systems that helps us emphasize the cognitive components of interfaces themselves. It helps us look at the things we design as extensions of the user’s own cognition. We can view anything from notes on a desktop to the entire internet as an extension of the user’s own memory.

**[GOAL #2] Students will understand the role of social cognition in interface design.**



*   Distributed cognition is concerned with how the mind can be extended by relations with other artifacts and individuals. Because we’re interface designers, we probably focus most of our time on the artifacts part of that. After all, artifacts are what we’re designing.
*   But the other part of distributed cognition, distributing across individuals, presents a powerful opportunity as well: the social portion of distributed cognition is concerned with how social connections create systems that can together accomplish tasks.
    *   It’s also concerned with the cognitive underpinnings of social interactions themselves: it’s interested in how perception, memory, and learning relate to social phenomena

**[GOAL #3] Students will understand the perspective of situated action in designing interactions and its focus on acting in the world.**



*   Like distributed cognition, situated action is strongly concerned with the context within which people interact.
*   But unlike distributed cognition, situated action is not interested in the long-term enduring or permanent interactions amongst these things.
*   That’s not to say the theory denies the existence of long-term memory, but it just has a different focus: **situated action** focuses on humans as improvisers.
    *   It’s interested not in the kinds of problems that people have solved before, but in the kinds of novel, situational problems that arise all the time.
    *   That's an important view for us to hold as interface designers.
    *   While we like to think we’re in charge of structuring the task for our users, in reality the task that they perform is going to grow out of their interaction. We can try our best to guide it in certain directions, but until the user gets their hands on it, the task doesn’t exist. And once they get their hands on it, the task is what they do, not what we designed.
*   **Takeaways**
    *   We must examine the interfaces we design within the context in which they’re used.
    *   We must understand that the task that users perform grows out of their interaction with our interfaces -- we don’t define it.
    *   We can try to structure it as much as we can, but until the users get started, the task doesn’t exist -- and once they get started they play a significant role in defining the task.
*   Situated action gives us a valuable lens especially to examine issues of memory.
    *   We mention in our lessons on memory and on design principles that recognition is easier than recall.
    *   People have an easier time recognizing the right answer or option when they see it than recalling it from scratch.
    *   That’s in part because memory is so context-dependent. Recognition provides the necessary context to identify the right option. Relying on recall means there’s little context to cue the right answer to the user.
*   Lucy Suchman’s 1985 book Plans and Situated Actions is the seminal book on the philosophy of situated action.
    *   The book is a detailed comparison between two views of human action:
        *   The first, she writes, “views the organization and significance of action as derived from plans”. And this is the model we very often adopt when developing interfaces: users make plans and carry out those plans.
        *   The second treats plans as derivative from situated action. In this view, people simply act in the world, and plans are what we derive from those actions. Instead of plans dictating actions, plans are just interpretations of actions.
    *   What this means for us as interface designers is that rather than assuming the user has a plan in mind that they are actively carrying out, we might consider viewing only their immediate interaction with the current screen. In other words, forget the history of actions that led a user to a screen, and ask just: once they’re here, how do they know what to do next?
    *   Later in this book, Lucy Suchman specifically touches on communication between humans and machines.
    *   There’s a lot more depth here as well. The key takeaway for us is the _focus on the resources available to the user at any given time_, but I do recommend reading the book and this chapter for more insights.

**[GOAL #4] Students will understand the pertinent takeaways of activity theory for HCI.**

*   Activity theory is a massive and well-developed set of theories regarding the interactions between various pieces of an activity. It predates HCI, and in fact, activity theory is one of the first places the idea of interacting _through_ an interface came from.
*   In conversations about HCI, though, there are three main contributions of **activity theory** I’d like you to come away with:
    *   First, when we discuss designing tasks and completing tasks through an interface, we risk missing a key component: why.
        *   We could jump straight to designing the task, but why is the user completing the task in the first place?
        *   That has significant implications for our design.
        *   Activity theory generalizes our unit of analysis from the task to the activity.
        *   We’re not just interested in what they’re doing, but why they’re doing it and what it means to them. Our designs will be different, for example, if users are using a system because they’re required to or because they choose to.
    *   Second, activity theory puts an emphasis on the idea that we can separate low-level operations from higher-level actions.
        *   This had a special historical significance: before activity theory and similar theories reached HCI in the 1980s, HCI was largely concerned with minute things like how quickly a person could click a button or type in a command.
        *   Activity theory helped us zoom out from those low-level interactions to general user needs.
    *   And third, activity theory points out that actions by the user can actually move up and down in this hierarchy.
        *   A common example of this is driving a car.
        *   The first time you drove a car, shifting gears between park and drive was a very conscious action. You had to think about how to press the button, which way to push the stick, and when to release it.
        *   However, after driving a few times it becomes second nature. It has shifted from shifting gears being a conscious goal to it simply being an operator in your driving behavior.
*   In 1996, Bonnie Nardi edited a prominent book on the study of context in human-computer interaction, titled Context and Consciousness. Two papers in particular stand out to me, both by Nardi:
    *   The first is a short paper that serves in some ways as an introduction to the book.
        *   Here, Nardi outlines the general application of activity theory to HCI.
        *   “Activity theory is a powerful and clarifying descriptive tool rather than a strongly predictive theory.”
        *   She emphasizes activity theory as a descriptive theory for understanding the way systems work, rather than a way of dictating the way people should work. She also notes the relevance of activity theory to HCI.
            *   “Activity theory offers a set of perspectives on human activity and a set of concepts for describing that activity” and “This … is exactly what HCI research needs as we struggle to understand and describe “context”, “situation”, “practice”.
            *   She notes that the theory is uniquely suited to addressing some of the interesting issues facing HCI in 1996.
            *   It’s also fascinating to view from a historical perspective: today, we understand the role that context has grown to play, especially with emerging technologies. It’s fascinating to me to look back at how the community was constructing that debate 20 years ago.
    *   From her volume Context and Consciousness, Nardi wrote a comparison between distributed cognition, situated action, and activity theory:
        *   “Attention to the shaping force of goals in activity theory and distributed cognition... contrasts with the contingent, responsive, improvisatory emphasis of situated action.”
        *   First, she notes that activity theory and distributed cognition are driven by goals, whereas situated action deemphasizes goals for a focus on improvisation: “Goals are our musings out loud about why we did something after we have done it”
            *   She goes on to summarize that situated action says goals are constructed retroactively to interpret our past actions.
        *   Nardi also evaluates the role of permanent, persistent structures, noting they are important for activity theory and distributed cognition, but present a tension for situated action.
            *   Again see a similarity between activity theory and distributed cognition.
                *   Nardi writes that the main difference between these two philosophies is their evaluation of the symmetry between people and artifacts.
                *   “Activity theory, with its emphasis on... motive and consciousness... sees artifacts and people as different.” Activity theory regards them as fundamentally different, given that humans have consciousness.
                *   “Distributed cognition... views people and [artifacts] as conceptually equivalent... “agents” in the system.” Distributed cognition, by contrast, believes that artifacts can serve cognitive roles, and those should be considered conceptually equivalent.



---


**3.6 Evaluation**

**[GOAL #1] Students will understand the role of evaluation in the design life cycle.**



*   The heart of user-centered design is getting frequent feedback from users.
*   **Evaluation**: we take what we’ve designed and put it in front of users to get their feedback.
    *   But just as different prototypes serve different functions at different stages of the design process, so also our methods for evaluation need to match as well.
    *   Our goal is to apply multiple evaluation techniques to constantly center our designs around the user. That’s why evaluation is a foundation of user-centered design -- just like we wanted to understand the user and the task before beginning to design, we also want to understand how the user relates to the design at every stage of the design life cycle.
    *   **To succeed in HCI, you need a good evaluation plan.**
        *   In industries like healthcare and education, that’s initially going to involve getting some time with experts outside the real context of the task. Bringing in doctors, nurses, and patients, and exploring their thoughts on the prototypes you’ve designed.
        *   In some places, like education, you might be able to evaluate with real users even before the interface is ready, but in others, like healthcare, the stakes are high enough that you’ll only want real users using the interface when you’re certain of its effectiveness and reliability.
        *   In some emerging areas, you’ll also be fighting multiple questions in evaluation.
            *   Take virtual reality, for example: most people you encounter haven’t used virtual reality before. There is going to be a learning curve.
            *   How are you going to determine whether the learning curve is acceptable or not? If the user runs into difficulties, how can you tell if those come from your interface or if they’re part of the fundamental VR learning experience?

**[GOAL #2] Students will understand when each type of evaluation is relevant.**



*   The type of evaluation we employ is **tightly related to where we are in our design process**.
*   **Qualitative evaluation:** evaluation that emphasizes the totality of a phenomenon
    *   Early on, we want more qualitative feedback.
        *   We want to know what they like, what they don’t like, whether it’s readable, whether it’s understandable.
    *   Later on, we want to know if it’s usable.
        *   Does it minimize their workload? Is it intuitive? Is it easy to learn? What’s hard? What were you thinking while using this interface? What was your goal when you took that particular action?
    *   The methods we use for qualitative evaluation are very similar to the methods we used for needfinding: **Interviews, think-aloud protocols, focus groups, surveys, post-event protocols**: we used these methods to get information about the task in the first place, and now we can use these techniques to get feedback on how our prototype changes the task.
    *   **Questions to ask when designing a qualitative evaluation**
        *   **1. Is this based on prior experience, or is it based on a live session?**
            *   If you’re doing a qualitative evaluation on an interface that the user has used in the past, then generally you’re probably doing needfinding: you’re looking for weaknesses in an already-in-use interface to improve.
        *   For the rest of these questions, we’ll assume that you’re evaluating an interface the user is about to see for the first time.
        *   **2. Is this synchronous or asynchronous?** Are you going to watch the participant live, or are they going to complete the evaluation on their own?
            *   Synchronous is usually beneficial because we can see a much greater amount of the interaction that is taking place.
            *   However, asynchronous is often easier to carry out, especially with larger populations.
            *   I’d generally recommend synchronous whenever possible, but asynchronous is better than nothing.
        *   **3. How many prototypes will they be evaluating? **You might have users evaluate only one prototype, or you might have them look at multiple.
            *   If you have them look at multiple, you want to make sure to vary the order between different participants: otherwise you might get consistently different feedback just because the user is already familiar with the problem domain when they get to a second interface.
        *   **4. When do you want to get feedback from the user?**
            *   There are two main protocols: think-aloud and post-event.
            *   With a think-aloud protocol, you have users think out loud while they’re using the interface.
            *   This is good because you get to find out exactly what they’re thinking and seeing. However, thinking aloud tends to bias the way users use the interface.
            *   They may act more deliberately and less intuitively, or put forth greater effort.
            *   Post-event protocols ask the user to wait until the end to give their feedback, which can lead to more natural interactions while they play with the interface.
            *   However, note that users are often subpar at explaining why they like or dislike things or why they made certain decisions, so you shouldn’t take everything the user says at face value.
        *   **5. Do you want this feedback from individuals or from groups?**
            *   Focus groups are used when multiple users talk together about their experiences. This can lead to better explanations, but it can also bias users toward the stronger personalities in the group.
            *   Individual interviews or surveys force the user to be the only source of knowledge, but that means the user isn’t biased by other outside views.
            *   As has become a trend, you’ll notice there are strengths and weaknesses with every approach, so we should vary what we do often.
    *   **Capturing qualitative evaluation**
        *   With qualitative research, we want to capture as much of the session as possible because things could come up that we don’t anticipate.
        *   When selecting a way to capture your qualitative evaluation, ask yourself: will my subjects find the camera intrusive? Am I capturing what happens on screen? How difficult will this data be to analyze?
        *   **Actually record the session with video.**
            *   **Pros**: automated/automatic, captures everything/comprehensive, allows us to focus on administering the session instead of capturing it (passive)
            *   **Cons**: It’s intrusive, it doesn’t capture on-screen interactions very well (screen-less), and it’s difficult to analyze. Someone has to watch it.
            *   Some of these issues can be resolved -- we can sync the video capture with screen capture, for instance.
            *   However, if we’re dealing with children, at-risk populations, or delicate subject matter, the intrusiveness of a camera can be overwhelming.
        *   **Note-taking**
            *   **Pros**: Cheap and easy, non-intrusive, easier to analyze than video
            *   **Cons**: Slow, not comprehensive (limited), interferes with administration (manual)
            *   If you’re going to use note-taking as your method for capturing qualitative evaluation, I recommend you have two people: a note-taker and an administrator. That lets each focus on their own responsibilities.
        *   **Software logging: **If we’re designing software, actually log the behavior in the software.
            *   This is in some ways the best of both worlds.
            *   **Pros**: Automatic/automated, allows us to focus on administering the session (passive), non-intrusive, analyzable
            *   **Cons**: Not as comprehensive (limited), very low-level (narrow), requires working software (tech-sensitive)
    *   **Tips for conducting successful evaluations**
        *   1. **Run pilot studies**. Recruiting participants is hard. You want to make sure that once you start working with real subjects, you’re ready to gather real useful data. So, try out your experiment with friends, family, or coworkers before trying it with real subjects to iron out the kinks in your design and directions.
        *   2. **Focus on feedback**. It’s tempting in qualitative evaluations to spend too much time trying to teach this one user. If the user criticizes an element of the prototype, you don’t need to explain to them the rationale. Your goal is to get feedback to design the next interface, not to teach the current user.
        *   3. **Use questions when users get stuck**. That way, you get some information on why they’re stuck and what they’re thinking. Those questions can also be used to guide users to how they should use it to make the session less instructional.
        *   4. **Instruct users what to do, but not how to do it**. This doesn’t always apply, but most often we want to design interfaces that users can use without any instruction whatsoever. So, in performing qualitative evaluation, give them instruction on what to do, but let them try to figure out how to do it. If they try to do it differently than you expect, then you know how to design the next interface.
        *   5. **Capture satisfaction**. Sometimes we can get so distracted by whether or not people can use our interface that we forget to ask whether they like using our interface! So, make sure to capture user satisfaction in your qualitative evaluation.
*   **Empirical evaluation: **evaluation based on numeric summaries or observations of a phenomenon
    *   Most empirical evaluations are comparisons
    *   This is where we actually do some controlled experiments and evaluate the results quantitatively.
    *   For that, we need many more participants, and we also want to make sure we’ve addressed the big qualitative feedback first.
    *   Then at the end, we might want to know something more quantitative.
    *   We might want to actually measure whether the time to complete a task has changed or whether the number of sales has increased.
    *   Goal: strong conclusions!
    *   **Designing an empirical evaluation**
        *   We have multiple conditions, which we call **treatments**.
            *   These treatments could be different interfaces, different designs, different colors, whatever we’re interested in investigating.
            *   Our goal here is to investigate the comparison between the treatments, and end up with a conclusion about how they’re different.
            *   However, we have to be careful to make sure that the differences we observe are really due to the differences between the treatments, not due to other factors.
                *   For example, imagine we were testing the difference between two logos, and we wanted to know what worked better: orange or teal.
                *   However, we also make one a circle while the other is a triangle. In the end, we wouldn’t be able to comment on orange vs. teal, we could only comment on orange circle vs. teal triangle.
                *   To make a judgment about the color, we need to make sure the color is the only thing we’re comparing.
                *   Of course, here, this sounds silly. In practice, though, differences can be more subtle. If you were testing different layouts, you might miss that one loads a bit faster, or one uses prettier images.
        *   Once we’ve designed the treatments, it’s time to design the experiment.
            *   Our first question is: what do participants do? Does each participant participate in one treatment, or both?
                *   If each participant only participates in one treatment, then our next step is easy: we split the randomly participants into two groups, and one-by-one, we have them go through their treatment. At the end, we have data from participants in one condition and data from participants in the other, and we can compare them. This is called **between-subjects** design.
                *   We can also do a **within-subjects** experiment. With a within-subjects experiment, each participant participates in both treatments. 
                    *   Within-subjects is beneficial because it allows us to gather twice as much data if our participant pool is limited: here, each interface would be used by 16 participants instead of just 8.
                    *   It also allows us to do within-subjects comparisons, seeing how each individual participant was affected instead of the groups as a whole. That can help us identify some more subtle effects, like if different people had different strengths.
                    *   However, within-subjects requires more of our subjects’ time, which can be a big problem if the treatments are each long.
                *   However, a major lurking variable could potentially be which treatment each participant sees first, so we still have to randomly assign participants to treatment groups. But instead of assigning participants to which treatment they’re receiving, we’re **randomly assigning** them to what order they’ll receive the treatments in. That way, if the order that participants receive the treatments matters, we’ll see it.
                    *   Throughout this example, we’ve also glossed over an important detail: random assignment. Random assignment to treatments help us control for bias. Imagine if all the smartest participants, or all the women, or all the punctual participants were assigned to the same treatment. That would clearly affect our results.
                    *   So, we randomly assign people to groups. That might also sound obvious, but imagine if your treatment involved a lot of physical set-up. It would be tempting to run the first eight participants on one set-up, and the second eight on the other. But what if that means the more punctual participants were all in the first condition? Or what if you got better at administering the experiment during the first condition, so that participants in the second condition had a generally smoother experience? All of these are lurking variables that are controlled by random assignment.
    *   **Hypothesis testing**
        *   Did results arise by random chance?
        *   Are results different enough to conclude that they’re really different?
        *   **Null and alternative hypotheses**
            *   Null hypothesis: we initially hypothesize that two things are equal.
            *   The alternative hypothesis is that they are not equal.
            *   We want to see if the difference is big enough to accept the alternative hypothesis rather than the null hypothesis; we generally do that if there is less than a 5% chance that the difference could have arisen by random chance.
            *   In that case, we say that the difference is “statistically significant”.
        *   This is the general process of hypothesis testing: assuming things are the same, and seeing if the data is sufficient to prove they’re different.
        *   **The specific kind of hypothesis test you conduct depends on the kind of data that you have.**
            *   For **nominal data**, we generally want to use a **Chi-square test**.
                *   A Chi-square test checks to see if the distribution of values to a number of buckets is the same across two conditions.
                *   Here, our independent variable is the conditions, treatments, etc., and our dependent variable is the distribution of values.
                *   Our null hypothesis is that the distributions are not different, and our alternative is that they are different.
            *   For **ordinal data**, our process is largely the same (**Chi-square test, Kolmogorov-Smirnov test, median test**)
                *   Our independent variable are the conditions or treatments, and our dependent variable is the distribution of values across those categories.
                *   Our null hypothesis is still that there are no differences, and our alternative is that there are differences.
                *   For that reason, we can use the same Chi-square test on ordinal data that we used on nominal data.
                *   However, that isn’t the absolute best thing to do: a Chi-square test doesn’t understand that the values are ordered, and so it doesn’t understand that a systematic shift across the categories is probably more notable.
                *   So, we might instead use a test called the Kolmogorov-Smirnov test, which acts similar to a Chi-square test but with additional knowledge of the order of the categories.
                *   We might also use a median test, which tests to see whether the medians of the samples are the same.
                *   It’s also not uncommon to make the assumption that the ordinal data is actually interval data and use the tests specifically developed for interval data; this isn’t a good thing to do because it assumes that the ordinal categories are evenly spaced, but it’s still not uncommon to do so because of the relative ease of the test.
            *   For **interval and ratio data**, the types of tests we use shift.
                *   If we’re comparing between only two treatments or conditions, then we’ll use what’s called a **Student’s t-test**.
                    *   A t-test lets us straightforwardly compare the means of two samples to see if there’s a significant chance that they’re different.
                    *   Here, our independent variable is whatever is different about the samples, and our dependent variable is the observed values.
                    *   Our null hypothesis is that the samples aren’t different, and our alternative is that they are.
                    *   This same test can be used for interval and ratio data because it isn’t dependent on there existing an absolute 0 point.
                    *   It’s worth noting that we’re only supposed to use t-tests when the data distribution is normal. If it isn’t, we should use things like the Mann-Whitney U test or the Kruskal-Wallis H test… but those topics are out of our scope. Generally, a Chi square test and a t-test will get you most of what you need… at least until we have more than three treatments.
                *   What if we wanted to test three interfaces at the same time?
                    *   You might be tempted to just do three t-tests or three Chi-square tests, one on each pair. This is called **repeated testing**, and the problem is that it raises the likelihood that we’ll find something that isn’t actually true, which is called a Type I Error.
                    *   **Type 1 error**: also known as a false positive - occurs when a researcher incorrectly rejects a true null hypothesis. This means that your report that your findings are significant when in fact they have occurred by chance
                    *   Remember, we said that we reject a null hypothesis if there’s less than a 5% chance that the data could have been so different due to randomness alone. That means that every time we run an additional test, we’re taking a 1-in-20 chance of finding something that isn’t there. So instead, we need a single test that can evaluate multiple conditions at once.
                    *   **Chi-square test**
                        *   For nominal and ordinal data, fortunately a Chi-square test can natively handle more than two conditions: we just have to put those conditions in.
                        *   The weakness here is that it doesn’t tell us what is different; it merely tells us that there exist differences.
                        *   So, if the Chi-square test on all the groups shows a difference, it’s acceptable to then follow-up with pairwise Chi-square tests between the conditions to drill into what the difference actually is.
                    *   For interval and ratio data, however, we need to use a different test altogether: **an Analysis of Variance, or ANOVA**.
                        *   A one-way ANOVA test lets us compare between three or more groups simultaneously. So, we could test between three or four different interfaces at the same time.
                        *   We can even do something called a two-way ANOVA, where we look at two independent variables at the same time. We could check three interfaces against two genders to see if performance is a function of both gender and interface selection.
                        *   So, ANOVA is what we use when we have more than two categories and interval or ratio data.
                    *   Notice, however, that there’s still one assumption in every test we’ve looked at so far: we’ve always assumed that the independent variable is nominal. What if the independent variable was instead interval or ratio? We might want to look at the relationship between time using an interface and speed completing a certain task to understand the learning curves for different interfaces.
                        *   We could do this by breaking the interval data into categories, but we could also do this by performing a regression. **Regression analyses** are used when both the independent and dependent variables are interval or ratio data.
            *   **Binomial data**
                *   A **binomial test** is used when we have binomial data. Binomial data is data with only two possible outcomes, like a coin flip. For us, we might have outcomes like ‘success’ or ‘failure’. We could be curious which interface allows a user to succeed at a task with greater frequency.
                *   What can be tricky here is that our data actually looks continuous: we might say “in Treatment 1, users succeed 55.4% of the time, while in Treatment 2, users succeed 46.5% of the time.” That looks continuous.
                *   Due to some details of how the math works, though, we don’t use standard t-tests with binomial data: instead, we use specific binomial t-tests.
                *   There are two kinds: with a one-sample binomial t-tests, we test to see if a distribution is different from some arbitrary number. For example, we could test a probability against random chance by testing it against 0.5.
                *   With a two-sample binomial t-test, we test to see if two distributions are different. So, we could test to see if one interface leads to success more often than the other.
            *   Even here, we’re only scratching the surface of the tests that are out there. There are also paired tests, repeated measures tests, tests with one tail vs. two tails… but that’s all advanced stuff. 
    *   **Tips for doing empirical evaluations**
        *   1. **Control what you can, document what you can’t**. Try to make your treatments as identical as possible. However, if there are systematic differences between them, document and report that.
        *   2. **Limit your variables**. It can be tempting to try to vary lots of different things and monitor lots of other things, but that just leads to noisy, difficult data that probably will generate some false conclusions. Instead, focus on varying only 1 or 2 things, and monitor only a handful of things in response. There’s nothing at all wrong with only modifying one variable and only monitoring one variable.
        *   3. **Work backwards in designing your experiment**. A common mistake I’ve seen is to just gather a bunch of data and figure out how to analyze it later. That’s messy and doesn’t lead to very reliable conclusions. Decide at the start what question you want to answer, then decide the analysis to use, then decide the data to gather. 
        *   4. **Script your analyses in advance**. Ronald Coase once said, “If you torture the data long enough,  nature will always confess.” What the quote means is that if we analyze and reanalyze data long enough, we can always find conclusions, but that doesn’t mean they’re actually there. So, decide in advance what analysis you’ll do, and do it -- if it doesn’t give you the results you want, don’t reanalyze it until it does.
        *   5. **Pay attention to power**. Power refers to the size of a difference that a test can detect, and generally it’s dependent on how many participants you have. If you want to detect a small effect, then you’ll need a lot of participants. If you only care about detecting a big effect, you can get by with fewer.
*   **Empirical Tests (IV = Independent Variables, DV = Dependent Variables)**

*   **Predictive evaluation: **evaluation based on systematic application of pre-established principles and heuristics
    *   Predictive evaluation is specifically evaluation without users.
    *   In user-centered design, this is obviously not our favorite kind of evaluation.
    *   Evaluation with real users, though, is oftentimes slow, so it’s useful for us to have ways we can do some simple evaluation on a day-to-day basis. Predictive evaluation is alright to use as part of a rapid feedback process. It lets us keep the user in mind even if we’re not bringing the user into the conversation.
    *   Along the way, we might also want to iterate more quickly by predicting what the results of the user evaluation will be.
    *   The important thing is to make sure we’re using it appropriately: predictive evaluation shouldn’t be used where we could be doing qualitative or empirical evaluation; it should be used where we wouldn’t otherwise be doing any evaluation.
    *   **Types of predictive evaluation**
        *   **Heuristic evaluation**: simply to hand our interface and these guidelines to a few experts to evaluate.
            *   Each individual evaluator inspects the interface alone, and identifies places where the interface violates a heuristic.
            *   We might sit with the expert while they perform the evaluation, or they might generate a report.
            *   Heuristics are useful because they give us small snapshots into the way people might think about our interfaces.
            *   If we take those heuristics to an extreme, though, we could go so far as to develop models of the way people think about our interfaces.
        *   During our needfinding exercises, we developed a model of our users’ tasks. **Model-based evaluation**: we take that model and trace through it in the context of the task.
            *   Let’s use a GOMS model as an example.
            *   Just as we computed a GOMS model for what users did in some context, we can compute a GOMS model for what they will do in our new interface.
            *   Then, we can compare these models side-by-side to see how our interface changes the task and evaluate whether it aids efficiency.
            *   We can also use our models of user profiles to evaluate whether the new design meets these criteria.
            *   If we take model-based evaluation to an extreme, we can actually get to the point of simulation-based evaluation.
        *   **AI agents**
            *   At that point, we might construct an artificially intelligent agent that interacts with our interface the way a human would. Ivory and Hearst did some research on such automated evaluation back in 2001.
            *   More recently, work has been done to create even more human-like models of users, like some work done by a research group at the Human-Centered Design Group at the Institute for Information Technology in Germany.
            *   Developing that agent is an enormous task on its own, but if we’re working on a big long-term project like Facebook or in a high-stakes environment like air traffic controlling, having a simulation of a human that we can run hundreds of thousands of times on different interface prototypes would be extremely useful.
        *   **Cognitive walk-through**: most likely the most common type of predictive evaluation you’ll encounter
            *   In a cognitive walkthrough, we step through the process of interacting with an interface, mentally simulating at each stage what the user is seeing, thinking, and doing.
            *   At every stage of the process, I want to investigate this from the perspective of the gulfs of execution and evaluation.
                *   Is it reasonable to expect the user to cross the gulf of execution? Is the right action sufficiently obvious? Is the response to the action the one the user would expect?
                *   Is it reasonable to expect the feedback to cross the gulf of evaluation? Does the feedback show the user what happened? Does the feedback confirm the user chose the right action?
            *   The weakness of cognitive walkthroughs is that we’re the designers, so it likely seems to us that the design is fine. After all, we designed it.
            *   But if you can sufficiently put yourself in the user’s shoes, you can start to uncover some useful takeaways without involving real users.
*   **Evaluation timeline**
    *   When we discussed prototyping, we talked about how over time, our prototypes will get higher and higher fidelity. Something similar happens with evaluation: ⇒ **Over time, our evaluation methods will change**. The evaluations we use early on differ from what we likely use later.
    *   **Purpose: Formative vs summative**
        *   Throughout most of the design process, our evaluations are **formative**, meaning their primary purpose is to help us redesign and improve our interface.
        *   At the end, though, we might want to do something more **summative** to conclude the design process, especially if we want to demonstrate that the new interface is better.


    *   **Approach: Qualitative vs empirical vs predictive**
        *   Our early evaluations are likely to be more interpretive, qualitative, and informal. Their goal is to help us improve.
        *   Our later evaluations are likely to be more empirical, controlled, and formal. Their goal is to demonstrate or assess change.

           

    *   **Data: Qualitative vs quantitative**
        *   Our early data is likely to be more qualitative, while our later data is likely to be more quantitative.


    *   **Setting: Lab testing vs field testing**
        *   Where the evaluation takes place: in the controlled lab environment or in the field.
        *   Generally, when we’re testing early interfaces, we probably want to do it in more of a lab setting, not live.
        *   Later on, we might do real live field testing.
        *   That lets us focus exclusively on the interface early on, then transition to focusing on the interface in context later.

            


    *   Of course, none of these are hard and fast rules: we’ll likely do some qualitative evaluation later sometimes, or some field testing early, but, in general, this is likely the order we’re likely to follow.
*   **Steps to ensure your evaluation is useful**
    *   Regardless of whether we’re doing qualitative, empirical, or predictive evaluation, these steps remain the same. Those different types of evaluation just fill in the experiment we design, and inform the performance measures, data analysis, and conclusions.

        

    *   **1. Define the task you’re examining.**
        *   Depending on your place in the design process, this could be very large or very small.
        *   If we were designing Facebook, it could be as simple as posting a status update or as complicated as navigating amongst and using several pages.
        *   It could involve context and constraints, like taking notes while running or looking up a restaurant address without touching the screen.
        *   Whatever it is, we want to start by clearly identifying what task we’re going to investigate.
    *   **2. Define the performance measures.**
        *   How are we going to evaluate the user’s performance?
        *   Qualitatively, it could be based on their own spoken or written feedback about the experience.
        *   Quantitatively, we could measure efficiency in certain activities or count the number of mistakes.
        *   Defining performance measures helps us avoid confirmation bias: it makes sure we don’t just pick out whatever observations confirm our hypotheses.
    *   **3. Develop the experiment.**
        *   How will we find users’ performance on the performance measures?
        *   If we’re looking qualitatively, will we have them think aloud while using the tool, or will we have them send in a survey after they’re done?
        *   If we’re looking quantitatively, what will we measure? What will we control, and what will we vary?
        *   This is also where we ask questions about whether our assessment measures are reliable and valid, and whether the users we are testing are generalizable.
    *   **4. Recruit participants.**
        *   As part of the ethics process, we make sure we’re recruiting participants who are aware of their rights and contributing willingly.
    *   **5. Do the experiment!**
        *   We have them walk through what we outlined previously.
    *   **6. Analyze the data.**
        *   We focus on what the data informs us about our performance measures.
        *   It’s important that we stay close to what we outlined initially -- it can be tempting to just look for whatever supports our design, but we want to be impartial.
        *   If we find some evidence that suggests our interface is good in ways we didn’t anticipate, we can always do a follow-up experiment to test if we’re right.
    *   **7. Summarize the data in a way that informs our ongoing design process.**
        *   What did our data say was working? What could be improved?
        *   The results of this experiment then become a part of our design life cycle.
        *   We investigated users’ needs, developed alternatives, made a prototype, and put the prototype in front of users.
        *   Based on this experience, we now have the data to develop a better understanding of the users’ needs, to revisit our alternatives, and to improve our prototypes by raising their fidelity for further testing and incorporating the lessons we just learned.

        

**[GOAL #3] Students will understand the type of data that comes out of each type of evaluation, and how it can be used.**



*   **Reliability**: whether or not some assessment of some phenomenon is consistent (returns the same results for the same phenomenon).
    *   In an assessment measure, we want it to be reliable across multiple trials. Otherwise its conclusions are random and not very useful
    *   _If we were to conduct the same procedure again, how likely is it that we’d get the same results?_ That’s reliability.
*   **Validity**: how accurately an assessment measures reality (if the results actually reflect the underlying phenomenon)
    *   An assessment could be completely reliable, but completely inaccurate.
    *   Validity is closely connected to a principle called generalizability.
    *   _How accurately does our data actually capture the real-world phenomenon we care about?_ That’s validity.
*   **Generalizability**: the extent to which we can apply lessons we learn in our evaluation to broader audiences of people (if the results can be used to predict phenomena beyond what it measured)
    *   For example, we might find the kinds of people that volunteer for usability studies have different preferences than the regular user. So, the conclusions we find in those volunteers might not be generalizable in measuring what we want to measure.
    *   _To what extent can we apply these conclusions to people that weren’t in the evaluation?_ That’s generalizability.
*   **Precision**: measure of how specific some assessment is (the level of detail a measurement supplies)
    *   _How specific are our conclusions and observations?_ That’s precision.
*   **Tips on what you might choose to evaluate**
    *   In designing evaluations, it’s **critical that we define what we’re evaluating**.
    *   Without that, we generally tend to bottom out in vague assessments about whether users like our interfaces or not.
    *   1. **Efficiency**. How long does it take users to accomplish certain tasks? This is one of the classic metrics for evaluating interfaces: can one interface accomplish a task in fewer actions or less time than another? You might test this with predictive models, or you might actually time users in completing these tasks. Still, this paints a narrow picture of usability.
    *   2. **Accuracy**. How many errors do users commit while accomplishing the task? This is typically a pretty empirical question, although we could address it qualitatively as well. Ideally, we want an interface that reduces the number of errors a user commits while performing a task. Both efficiency and accuracy, however, examine the narrow setting of an expert user using an interface. That brings us to the third metric...
    *   3. **Learnability**. Sit a new user down in front of the interface. Define some standard for expertise. How long does it take the user to hit that level of expertise? Expertise here might range from performing a particular action to something like creating an entire document.
    *   4. **Memorability**. Similar to learnability, memorability refers to the user’s ability to remember how to use an interface over time. Imagine you have a user learn an interface, then leave and come back a week later. How much do they remember? Ideally, you want interfaces that need only be learned once, which means high memorability.
    *   5. **Satisfaction**. When we forget to look for the other metrics, we bottom-out in a general notion of ‘satisfaction’, but that doesn’t mean it’s unimportant. We need to operationalize it, though. Experience is things like users’ enjoyment of the system or the cognitive load they experience while using the system. To avoid social desirability bias, we might want to evaluate this in creative ways, like finding how many participants actually download an app they tested after the session is over.
    *   Regardless of what you want to evaluate, it’s important that you very clearly articulate at the beginning: what you’re evaluating, what data you’re gathering, and what analysis you will use. These three things should match up to address your research questions.

**[GOAL #4] Students will understand each individual type of evaluation and what conclusions it can generate.**



*   See the giant outline above



---


**2.9 Interfaces and Politics**

**[GOAL #1] Students will understand the role that society plays in shaping interface design, and the role interfaces play in shaping society.**



*   In 1980, Langdon Winner published a highly influential essay in which he asked: Do Artifacts Have Politics? In other words, do technical devices have political qualities?
    *   When we say ‘politics’, we mean whether artifacts can personify specific forms of authority or power, whether for good or bad.
    *   What we’re referring to is the fact that artifacts or interfaces we design change the world around us just the way politicians or business interests do.
    *   Sometimes that’s by design. We might design interfaces not for usability or research, but to create change in the world.
    *   Other times, that social change happens in ways we didn’t anticipate. We design interfaces that are used and affect the world in ways we never anticipated.
    *   The paper describes numerous ways in which technologies, interfaces, and other artifacts demonstrate political qualities.
        *   “Reliance upon nuclear power... may be possible only in a totalitarian state.”
            *   For example, he opens by noting the belief that nuclear power can only be used in a totalitarian society because of the inherent danger of the technology.
        *   “dispersed solar sources are more compatible... with social equity, freedom, and cultural pluralism.”
            *   Solar, on the other hand, pushes society toward a more distributed and egalitarian structure. But of course, we understand that nuclear power isn’t authoritarian on its own.
        *   Winner is proposing that the push for certain technologies carries with it certain necessary political adjustments.
            *   That’s part of what it means to suggest that artifacts have politics.
            *   In the paper, Winner outlines two distinct ways in which artifacts have politics:
                *   **Inherently Political Technologies**
                    *   Like nuclear power, are technologies that are inherently political.
                    *   Certain technologies, whether due to complexity, safety, or resources, require considerable top-down organization. These lend themselves to authoritarian power structures.
                *   **Technical Arrangements as Forms of Order**
                    *   Technologies can also be used to achieve changes to social order when used in the correct way.
                    *   The technology itself has no inherent political leanings, but its use in a particular context accomplishes some.
                    *   Winner uses the example of a factory in Chicago in the 1880s that replaced workers with automated machines that produced inferior goods as a way of busting up the union. The new technology was inferior, but was used to serve a political purpose.
                *   So, according to Winner, artifacts may have two kinds of politics: they may be inherently political, or they may be used to achieve political motives.
*   **The three goals of HCI:**
    *   **1. Help a user do a task.**
        *   Most commonly in HCI, we’re interested in designing for usability. We want to make tasks easier through technology.
            *   So in a car, we might be interested in designing a GPS that can be used with the fewest taps or a dashboard that surfaces the most important information at the right time.
    *   **2. Understand how a user does a task.**
        *   Sometimes, we’re also interested in designing for research.
            *   We might design a dashboard that includes a visualization of the speed to see if it changes how people perceive how fast they’re going.
    *   **3. Change the way a user does a task due to some value that we hold, like safety or privacy.**
        *   A third motivation is to somehow change the user’s behavior, designing for change in response to some value.
            *   And oftentimes that may actually conflict with the other motivations.
            *   If we’re trying to discourage an unhealthy habit, we want to make the interface for that habit less usable.
            *   Cars have lots of interfaces created with this motivation.
                *   If I start driving without a seatbelt on, my car will beep at me.
                *   Some cars will cap your speed.
            *   Those interfaces serve no usability goals, but rather they serve the goal of user safety.

**[GOAL #2] Students will understand the nature of intentional and unintentional repercussions of interface design.**



*   **Negative change by design**
    *   The ability of interfaces to change behavior can be abused.
    *   We’re not just talking about places where people put explicit barriers up, like blocking people from accessing their content.
    *   There are instances where people create seemingly normal designs with underlying political motivations.
    *   Winner describes one such instance in his essay, “Do Artifacts Have Politics?”
        *   Robert Moses was an influential city planner working in New York City in the early 1900s.
        *   As part of his role, he oversaw the construction of many of the beautiful parks on Long Island. He also oversaw construction of parkways, roads to bring the people of New York to the parks. And as part of that, he oversaw the construction of overpasses to connect local roads over the parkways.
        *   But something unfortunate happened. These bridges were too low for buses to pass under them.
        *   As a result, public transportation couldn’t easily run to his parks. And as a result of that, only people wealthy enough to afford cars of their own could come to his parks.
        *   An unfortunate coincidence, right? The evidence shows it’s anything but a coincidence. Moses intentionally constructed those bridges to be too low for buses to pass under them as a way of keeping poor people out of his parks. His political motivations directly informed the design of the infrastructure, and the design of the infrastructure had profound social implications.
    *   One of the arguments from proponents of net neutrality is that without it, companies could set up fast lanes that prioritize their own content, or worse, severely diminish content of their competitors or content critical of the company.
*   **Positive change by design**
    *   We can design for positive social change as well, though.
    *   This goes beyond just encouraging people to be nice or banning bad behavior.
    *   Interfaces can be designed that lead to positive social change through natural interaction with the system.
        *   One example I like of this is Facebook’s ubiquitous Like button.
        *   For years many people have argued for a Dislike button to complement the Like button. Facebook has stuck with the Like button, though, because by its design, it only supports positive interactions. It dodges cyber-bullying, it dodges negativity.
    *   For usability purposes, it’s a weakness because there are interactions I can’t have naturally in the interface -- but this wasn’t designed with usability in mind.
        *   Then, Facebook added five other emotions to the Like button, but notice that even these all have a positive connotation to them.
        *   For two of them it’s obvious: love and wow.
        *   Sadness and anger, however, within this context take on a more sympathetic connotation.
        *   When someone posts something sad, you don’t want to ‘like’ it, but you want to express support -- that’s what sadness does. When someone posts something that makes them angry, you too want to sympathize.
        *   In some instances, these might be used negatively -- if I post something about a political candidate and someone reacts with ‘Sad’ or ‘Angry’, I might perceive that as criticizing my opinion, but even then it’s not a direct mapping the way ‘Like’ is.
        *   So it seems clear that this interface was designed to foster positive social interactions online at the expense of the usability of similarly supporting all social interactions online.
    *   This also doesn’t have to be strictly about dictating change, but it can also be about supporting change.
        *   For example, until a few years ago, Facebook had a more limited set of relationship options: Married, Engaged, In a relationship, and It’s Complicated.
        *   When Facebook added domestic partnerships and civil unions, it didn’t magically create those constructs.
        *   However, it supported an ongoing societal trend and gave it some validity.
        *   The same can be said for its expanded gender options. It supports a diverse group of people feeling as if the interface is designed with them in mind, which in turn supports society’s general movement toward acceptance.
*   **Positive change by happenstance**
    *   Positive change doesn’t have to happen by design, though. In fact, there are numerous examples of positive change happening more as a byproduct of technological advancement than as a goal of it.
        *   In Bijker’s _Of Bicycles, Bakelights, and Bulbs_, this is the ‘Bicycles’ example.
        *   Bijker tells a story of a young woman bicycling in England. Because of the physical design of the bicycle, she is wearing knickerbockers rather than the more traditional skirt. She pulls up to an inn, but the proprietor refuses to serve her because she is dressed improperly.
        *   In fact, in those days, the very fact that she is out without her husband is significant as well. But the bicycle provided the independence she needed to go to the inn on her own, and it created the need for her to wear less formal attire. The bicycle was thus a driver in both providing her independence and prompting the change in her attire.
        *   Both those changes forced society to challenge traditional gender roles.
        *   Its role in women’s liberation was so significant that Susan B. Anthony once said, “I think [bicycling] has done more to emancipate women than anything else in the world.”
        *   But when the bicycle was invented, it’s doubtful that the inventors sat down and said, “Surely, this will free emancipate women and change our culture’s gender roles!” But yet, that is exactly what ended up happening.
*   **Negative change by happenstance**
    *   If we aren’t careful, we can also inadvertently create negative changes -- or further preserve existing negative dynamics -- with our artifacts as well.
        *   A good example of this is the proliferation of the internet in the first place.
        *   When the internet first came along, it piggybacked on existing phone lines.
        *   Then, it started piggybacking on more expensive cable TV lines.
        *   Now, it’s following along with very expensive fiber optic lines.
        *   At every stage of the process, areas with more well-developed infrastructure get the latest internet speeds first.
        *   However, generally the areas with well-developed infrastructure are the wealthier areas anyway, either because wealthier citizens paid for improved infrastructure or because wealthy people chose to move to those areas.
        *   High-speed internet access is a big economic boon, and yet areas that are already economically advantaged are generally the first ones to get higher-speed internet access.
        *   Even today, in poor parts of the United States, the only available internet connections are slow, unreliable satellite connections with strict data caps.
        *   In the rest of the world, the issue is even more profound.
        *   And yet, this isn’t intentional.
        *   Unlike the bridges on Long Island, no one is saying, “Let’s withhold broadband access from poor people to keep them poor!”
        *   Instead, it’s natural to install better connections where there is existing infrastructure to build on -- but that very natural plan has profoundly negative implications for equitable access to the internet.

**[GOAL #3] Students will understand the value of value-sensitive design.**



*   In HCI, we describe the idea of interfaces becoming invisible. Some of that is a usability principle, but it applies more broadly to the way that interfaces integrate themselves into our everyday lives. And if our interfaces are going to integrate into people’s lives, then they need to share the same values as those individuals as well. This connects to a field called Value-Sensitive Design.
*   The Value-Sensitive Design Lab at the University of Washington defines **Value-Sensitive Design** by saying: “Value sensitive design seeks to provide theory and method to account for human values in a principled and systematic manner throughout the design process.”
    *   In this way, value-sensitive design is another dimension to consider when designing interfaces: not only is an interface useful in accomplishing a task and usable by the user, but is it consistent with their values?
*   One of the most well-developed application areas of value-sensitive design is privacy by design.
    *   Privacy is a value, and privacy by design has aimed to preserve that value in the design of systems.
    *   It’s possible to design useful, usable interfaces that don’t take privacy into account.
    *   That’s what makes an examination of users’ values an extra dimension of interface design.
*   **Paper Spotlight: “Value Sensitive Design and Information Systems” by Batya Friedman**
    *   Batya Friedman is one of the co-directors of the Value Sensitive Design Research Lab at the University of Washington, and she co-authored one of the seminal papers on the topic: Value Sensitive Design and Information Systems
    *   Friedman, Kahn, and Borning have an excellent paper on the philosophy.
    *   In it, they cover three investigations for approaching value-sensitive design:
        *   **Conceptual questions**
            *   Conceptual investigations are thought experiments where we explore the role of values through questions like: &lt;read some questions>
        *   **Empirical investigation questions**
            *   Empirical investigations go out and use real users, exploring how they make sense of interfaces, and answering questions like: &lt;read some questions>
        *   **Technical investigations**
            *   Technical investigations are like empirical investigations that target the system instead of the user.
            *   They ask the same kinds of questions about the system, like: &lt;read some questions>
    *   **Highlight features as read**
        *   The paper also proposes some of the fundamental features of value-sensitive design, like: value-sensitive design should be proactive, and value-sensitive design distinguishes between usability and human values.
        *   If you’re planning to work in an area where human values play a significant role, and I would argue that is most areas, I highly recommend reading through this paper.
*   **Value-sensitive design across cultures**
    *   One of the challenges with value-sensitive design is that values can differ across cultures.
    *   The internet makes it technologically possible to design single interfaces that are used by people in nearly every country.
    *   But just because it’s technologically possible doesn’t mean it’s practically possible, and one reason for that is that different countries and cultures may have different values.
    *   A recent newsworthy example of this occurred with the Right to Be Forgotten.
        *   The Right to Be Forgotten is a law in the European Union that allows individuals some control over what information is available about them online. That’s a value held by the European Union.
        *   However, technologies like Google were not generally developed with that value in mind.
        *   So, it has been an extraordinary effort to try to technologically support that right to be forgotten while still providing search capabilities.
        *   But that value isn’t universally shared. Many people argue that the law could effectively become internet censorship. So now we start to see some conflict in values between different cultures.
    *   If we’re to design interfaces that can reach multiple cultures, we need to understand the values of those cultures, especially if it might force us to design different systems for different people in order to match their local values.
*   **Tips for incorporating value-sensitive design into your interfaces**
    *   1. **Start early**. Identify the values you want to account for early in the design process, and check on them throughout the design process. The nature of value-sensitive design is that it might have significant connections not just to the design of your interface, but to the very core of the task you’re trying to support.
    *   2. **Know your users**. In order to design with values in mind, you need to know your users’ values. Certain values are incompatible with one another, or at least present challenges for one another. Privacy as a value is in some ways in conflict with the value of record-keeping. To know what to design, you need to know your users’ values.
    *   3. **Consider both direct and indirect stakeholders**. We usually think about direct stakeholders -- those are the people that actually use the system. Value-sensitive design encourages us to think about indirect stakeholders, people that do not use the system but are nonetheless affected by it. When you’re designing the internal system for use by a bank, for example, it’s used by bank employees, but bank customers are likely impacted by the design.
    *   4. **Brainstorm the interface’s possibilities**. Think not only about how you’re designing the system to be used, but how it could be used. If you wanted to make a system that made it easier for employees to track their hours, consider whether it could be used by employers to find unjust cause for termination.
    *   5. **Choose carefully between supporting values and prescribing values**. Designing for change is about prescribing changes to values, but that doesn’t mean we should try to prescribe values for everyone. At the same time, there are certainly values held in the world that we would like to change with our interfaces if possible, with regard to issues like gender equality and economic justice. Be careful and be deliberate about when you choose to support existing values and when you choose to try to change them with your interfaces.
*   **Challenges for interface designers** when artifacts or interfaces have political clout
    *   First, we need to think about places where we can use interface design to invoke positive social change.
    *   Second, we also need to think about the possible negative ramifications of our interfaces: what undesirable stereotypes are we preserving, or what new negative dynamics might we create?
    *   Ask yourself:
        *   What role can your technology play in creating positive societal change?
        *   What risks are there if your technology catches on?

**[GOAL #4] Students will understand how interfaces can draw from incentives besides usability and research.**



*   Reversing the relationship
    *   Political relationships and motivations can often have enormous impact on technology, too.
        *   From Bijker’s book of Bicycles, Bakelights, and Bulbs, the ‘Bulbs’ part refers to the battle over the design of the first fluorescent light bulbs in 1938.
        *   General Electric created a new kind of light that was far more energy-efficient.
        *   Power companies, however, were afraid this would reduce power consumption and cut into their profits.
        *   After a long drawn out battle involving the Antitrust Division of the US government and the US Department of War, the fluorescent bulbs that were ultimately sold were not as good as they technologically could be in order to preserve others’ business interests.
    *   More and more we see compatibility between devices or usage policies for technologies determined not by what is technologically possible, but by what satisfies political needs.
        *   To keep up with everything that I like to watch on TV, I have five different subscriptions. I have my cable TV subscription. I have Hulu. I have Amazon Prime. I have Netflix. I have an HBO subscription separate on top of my cable subscription. And that’s not to mention things I watch for free on their own apps, like Conan or anything on YouTube.
        *   And you might think, wouldn’t it be awesome to just have one experience that could navigate among everything I want to watch? There’s no technological reason against it. But there’s a complicated web of ownership and licensing and intellectual property agreements that determine the way the technology works.
    *   Technology changes society, but society changes technology, too.
*   You have almost certainly experienced political or business motivations changing the way in which a technology of yours works.
    *   Similar to the fluorescent light bulb, oftentimes these motivations are to preserve the power or profit of an influential organization in the face of radical change.
    *   Sometimes they might be the products of relationships or agreements between vendors or organizations to emphasize one another’s content.
    *   Generally, these are instances where technology either performs sub-optimally or has certain features because someone besides the user benefits.
*   Notice how all these perspectives harken back to the idea that user experience exists not only in individuals and groups, but in societies.



---


**2.10 Conclusions to Principles**

**[GOAL #1] Students will solidify their knowledge of the three views of the human in HCI.**



*   One way of knitting together the different ideas of HCI is to start very close and zoom out.
    *   At the narrowest level, processor model. Zoom out - predictor model.
*   **Processor model**
    *   This view looks at this almost like an interaction between two computers -- one just happens to be a human, but the human’s actions are approached almost computationally.
    *   If we’re going to take this model, we need to understand a lot about what the human can sense, remember, and physically do.
    *   The **GOMS model** approaches HCI in this manner as well: It distills the human’s role out into goals, operators, methods, and selection rules, all of which can be externalized.
    *   But this is a pretty narrow view of HCI.
*   **Predictor model**
    *   For the most part, we’re interested in something more task-focused. In fact, this is where we likely spend the majority of our time. This is the user interacting through some interface to accomplish a task.
    *   This is what we meant by the ‘Predictor’ model. The user is actively involved in looking at their task and making predictions about what to do and what will happen.
    *   We look at questions like the **gulfs of execution and evaluation**, how hard it is for the user to interact with the task, and how hard it is for them to get feedback on their execution.
    *   The interface ideally can disappear from the interaction (**invisibility**), making the user feel as if they’re just working on the task directly. Many of our design principles are constructed specifically to help with this: to help users quickly make sense of the interface and understand the underlying task.
    *   But in order to design this interaction effectively, we have to understand the way the user thinks about the task they’re performing. We have to understand their mental models, and in turn, we have to help make sure their mental models match the actual task. Here we have to get into questions like understanding the **user’s errors** and understanding the **mapping between representations and the underlying tasks**.
    *   We also have to address questions like expert blindspot and learned helplessness.
    *   Fortunately, we have a tool to help us with this: **cognitive task analysis**, and its related **hierarchical task analysis**.
    *   So, much of what we deal with in HCI occurs within this process of a human completing a task through some interface.
*   **Participant view**
    *   We’re also interested in how this interaction occurs beyond just the individual and the interface. This is what was meant by the ‘participant’ model. The user is not merely interacting with an interface or interacting with a task through an interface. They are interacting with other interfaces, other individuals, and society as a whole. They are active participants in the world around them.
    *   Sometimes we’re interested not only in the task that the user is performing, but also in their motives and reasons to be performing it.
        *   This is what **activity theory** advocates: treating the unit of analysis not as a task, but as an activity, including some elements of the context surrounding the task.
    *   Sometimes we’re interested in how other artifacts or minds combine to help accomplish the task.
        *   That’s what **distributed cognition** advocates.
    *   Sometimes we’re interested in deeply understanding the situated context in which a person is acting.
        *   That’s what **situated action** is most interested in.
    *   Sometimes we’re interested in how these interfaces integrate with existing social norms and relationships.
        *   That’s what **social cognition** examines.
    *   And sometimes we’re interested in dynamics even broader than this.
        *   Sometimes we’re interested in how the interfaces we design can create **positive social change**.
        *   Sometimes we’re interested in how interfaces risk perpetuating existing **negative relationships in society**.
        *   **Equity, flexibility, ease & comfort** -- exactly the goal of some of our design guidelines as well: to use interfaces to create a more equal society for all people.

**[GOAL #2] Students will integrate the multiple lessons across the previous several videos into a unified understanding of HCI’s principles.**



*   When we started our conversations, I commented that when you do HCI right, users won’t actually know you’ve done anything at all. Good HCI disappears between the user and the task that they’re completing. As a result, people can underestimate how complex good HCI can be to leverage.
*   There are a lot of designs you could propose.
    *   But the question here isn’t what you designed. The question here should be, how will you develop it? How will you evaluate it? How do we know which ideas are good and which are bad?
    *   We’ve given some heuristics and principles for doing this, but that doesn’t automatically get you to a good interface. ⇒ That just establishes a baseline. That’s what that principles portion of this course covers.
    *   To fully develop interfaces using these principles, we need the methods of HCI as well.
*   We’re interested in general principles and methods for designing interactions between humans and various different kinds of computational devices.
    *   Designing for a traditional screen has its own set of specific principles.
    *   However, it’s quite likely that you’ll be designing for this traditional kind of device, so here are **tips for designing effective on-screen user interfaces**.
        *   1. **Use a grid**. Grids are a powerful way of guiding a user’s sight around your interface, highlighting important content, grouping related content, etc. There’s a reason why newspapers and magazines have used grid-based layouts for decades.
        *   2. **Use whitespace**. Users are good at consuming small chunks of information at a time. Notice how news articles often use very short paragraphs and highway signs have lots of space around the text. Whitespace works with grids to provide context and guide the user’s visual perception of the interface.
        *   3. **Know your Gestalt principles**. Gestalt principles in UI design refer to how users perceive groups of objects. Users group objects together when they’re spatially close together, visually similar, or moving together.
        *   4. **Reduce clutter**. The eye has difficulty processing cluttered information, so reduce clutter wherever possible. Grids, whitespace, and Gestalt principles can help with this because they can invisibly communicate content that might otherwise need to be communicated visually. Instead of drawing a box to group controls together, you can surround them with whitespace. Instead of using headers and text to label different portions of some content, you can separate them with a grid. And so on.
        *   5. **Design in grayscale**. Color can be a powerful tool, but it also runs awry of universal design. There are enough colorblind individuals in the world that relying on color is problematic. Color can help emphasize the content and structure of your interface, but it shouldn’t be necessary to understand it. Take a stoplight, for example: red means stop and green means go, which is a problem if you’re deuteranopic, or red-green color blind. But the red light is always at the top and the green light is always at the bottom, so if you are deuteranopic, you can still understand what the light is saying. Color emphasizes the content, but the content doesn’t rely on color.

**[GOAL #3] Students will understand the principles as merely half the picture of HCI.**



*   Design guidelines and heuristics and principles are only half the picture. They’re necessary for good design, but they aren’t sufficient. You can’t just grab these guidelines off the shelf, throw them at a new task, and expect to have a perfect interface the first time.
    *   These principles give you a solid foundation, but every domain, every task, every target audience has unique requirements and criteria.
    *   To design usable interfaces, you have to understand your specific user. And remember, that isn’t you.
    *   What that means is you have to go out and find what your users need.
    *   You have to get inside their head and understand the task. You have to prepare multiple different ideas for them to try out. You have to evaluate those with real users. And you have to take that experience and use it to improve your design, and start the process all over again.
*   These guidelines and principles are useful in making that process as efficient as possible. But they aren’t sufficient on their own. **These principles are only half the picture. The other half are the methods.**



---


**3.7 HCI and Agile Development**

**[GOAL #1] Students will understand what agile development is in the context of HCI.**



*   New technologies and new eras call for new principles and new workflows. Specifically, the advent of the internet ushered in new methods for HCI. Many software developers now adopt an **Agile workflow**, which emphasizes _earlier delivery, more continuous improvement, and rapid feedback cycles_.
    *   For those of us here in HCI, that’s exciting: we love feedback cycles. We love building them for our users and we love engaging in them ourselves.
    *   It’s also a scary prospect, though. We’ve also discussed long prototyping processes that move from paper to wireframes to live demos, that involve lots of users in slow qualitative methodologies. And those things are all still very valuable.
    *   But nowadays, sometimes we also need to build really fast.
*   Think back to before the age of the internet. Developing software was expensive. It required a very specialized skillset. Software distribution was done the same way we sold coffee mugs or bananas: you’d go to the store and buy it. That distribution method was expensive as well. If you shipped software that was hard to use, the cost of fixing that was enormous: you had to mail each individual person an update disk. And the only way to get user feedback and even find out if it was usable was the same way you would do it before distribution: by having users come in for testing. All of this meant there was an enormous need to get it right the first time.
    *   Shigeru Miyamoto, the creator of Nintendo’s best franchises, described this in terms of video games as, “A delayed game is eventually good, but a rushed game is forever bad.”
    *   The same applied to software.
*   Fast-forward to now -- is that still true?
    *   Development isn’t cheap, but it’s not as expensive as it used to be: a single person can develop in a day what would taken a team of people months to do 20 years ago, thanks to advances in hardware, programming languages, and available libraries.
    *   But more importantly, distribution for software is now essentially free. Updating software is essentially free as well. Every day, you can download new apps and have them update automatically in the background. If you release something with a bug, you can fix it and roll it out immediately.
    *   Miyamoto’s quote is no longer as accurate because it is possible to fix games after they’re released.
        *   Tesla regularly pushes software updates to its cars via the internet.
        *   And perhaps most importantly, we can gather usage data from live users automatically and for free. And it isn’t just usage data: product reviews, error reports, buzz on the internet. Lots of feedback comes naturally. What this all means is there is now more incentive to build something fast and get it to users to start getting real feedback as early as possible.
        *   This isn’t justification to just throw out the design life cycle. The majority of HCI design and research still goes through the longer process. You need several iterations through the full design life cycle for big web sites, complex apps, anything involving designing hardware, anything involving a high-profile first impression, or anything involving anything even somewhat high stakes.
        *   But that said, there exists a new niche for rapid development. Maybe you came up with an idea for a simple Android game. In the time it would take you to go through this longer process, you could probably implement the game and get it in front of users and get a lot more feedback.
        *   That’s what we’re discussing here ⇒ **How do you take the principles we’ve covered so far and apply them to a rapid, agile development process**?

**[GOAL #2] Students will understand when it is appropriate and inappropriate to leverage more agile development techniques.**



*   When should you consider these more Agile methodologies?
    *   Lots of software development theorists have explored this space. Boehm and Turner specifically suggest that **Agile development be used only in certain circumstances**:
        *   First, it must be **an environment with low criticality**.
            *   By its nature, Agile development means letting users do some of the testing, so you don’t want to use it in environments where bugs or poor usability are going to lead to major repercussions.
            *   Healthcare or financial investing, for example, wouldn’t be great places for Agile development generally speaking -- although there have been efforts to create standards that would allow the methodology to apply without compromising safety and security.
            *   But for things like smartphone games or social media apps, the criticality is sufficiently low.
        *   Second, it should be a **place where the requirements change often**.
            *   One of the benefits of Agile processes is they allow teams to adjust quickly to changing expectations or needs.
            *   A thermostat, for example, doesn’t change its requirements very often. A site like Udacity, though, is constantly adjusting to new student interests or needs.
        *   These two components apply to the types of problems we’re working on.
        *   If we’re working on an interface that would lend itself to an Agile process, we also must set up the team to work well within an Agile process. That means small teams that are comfortable with a culture of change, as opposed to large teams comfortable with order. So, generally, **Agile processes can be good in some cases and with the right people, but poor in others**.



*   **Paper Spotlight**: “Towards a Framework for Integrating Agile Development and User-Centred Design” by Stephanie Chamberlain, Helen Sharp, and Neil Maiden
    *   In 2006, Chamberlain, Sharp, and Maiden investigated the conflicts and opportunities of applying agile development to user-centered design. They found, interestingly, that the two actually had significant overlap.
        *   Both agile development and user-centered design **emphasize iterative development building on feedback** from the previous rounds.
        *   Both place a heavy emphasis on the **user’s role in the development process**.
        *   Both emphasize the **importance of team coherence**.
        *   So it seems that agile methods and user-centered design agree on the most fundamental element: the importance of the user.
    *   By comparison, the conflicts are relatively light:
        *   User-centered design disagrees with agile development on the importance of documentation
        *   Also disagrees on the importance of research prior to design work beginning.
    *   But clearly the methodologies have the same objectives; they just disagree on how to best achieve it.
    *   As a result, they propose five principles for integrating user-centered design into agile development.
        *   Two of these were shared between the methodologies in the first place:
            *   (1) **high user involvement**
            *   (2) **close team collaboration**.
        *   User-centered design’s emphasis on prototyping and the design life cycle showers up by proposing that designers run a sprint ahead of developers to perform the research necessary for user-centered design. To facilitate this, **strong project management** is necessary.

       
*   **A/B testing**
    *   So in some contexts, it’s now no harder to construct an actual interface than it is to construct a prototype. So, we might skip the prototyping phase altogether. However, prototypes also allow us to gather feedback from users. Even though we can now easily construct an interface, we don’t want to immediately roll out a completely untested interface to lots of people. We might be able to fix it quickly, but we’re still eroding user trust in us and wasting our users’ time.
    *   That’s where the second facet of this comes in: A/B testing - **rapid software testing between typically two alternatives**.
        *   Statistically, this is no different from t-tests.
        *   What makes A/B testing unique is that we’re usually rapidly testing small changes with real users. That way we can make sure a change is positive before rolling it out to everyone. But look where testing and feedback are coming in here: they’re coming automatically with real users during normal usage of our tool. There’s no added cost to recruiting participants, and the feedback is received instantly.
        *   With A/B testing, we’re making tiny little changes and observing the impact in terms of numerical outcomes like conversions, time on page, and so on. So, **A/B testing is reminiscent of the processor view** of the user.
*   **Agile development techniques don’t replace the design life cycle. They just caffeinate it.**
    *   We’re still doing needfinding, but we’re probably doing it pretty tacitly just by reading user feedback or checking out interaction logs.
    *   We’re still brainstorming design alternatives, but we’re really just coming up with them in our head.
    *   We’re still doing prototyping, our prototypes just happen to work.
    *   We’re still doing evaluation by rolling our changes out to only certain participants first and making sure the response is good. The results of that evaluation then feed the same process over again.
*   **Tips for using HCI and Agile development together**, especially for mitigating the risks to the user experience presented by Agile development.
    *   1. **Start more traditional**. Start with a more traditional needfinding and prototyping process, and shifting to more Agile development once you have something up and running. Jakob Nielsen describes this as doing some foundational user research. Once you have something up and running, you have a way of probing the user experience further, but you need something solid to begin with, and that comes from the traditional process.
    *   2. **Focus on small changes**. Notice that when I was doing live prototyping and A/B testing, I was making small changes to an existing interface, not building an entire site from scratch.
    *   3. **Adopt a parallel track method**. Agile development often uses short two-week sprints of development. Under that setup, have the HCI research one sprint ahead of the implementation. The HCI team can do two-week sprints of traditional needfinding, prototyping, and low-fidelity evaluation, then hand the results to the development team for their next sprint.
    *   4. **Be careful with consistency**. One of our design principles was consistency, both within our own interfaces and across interface design as a whole. If your interface caters to frequent visitors or users, you’ll want to be conservative in how often you mess with their expectations. If you’re designing something like a museum kiosk, though, you can be more liberal in your frequent changes.
    *   5. **Nest your design cycles**. In Agile development you go through many small design cycles rapidly, and each cycle gives you a little bit of new information. Take all that new information you gather and use it in the context of a broader, more traditional design cycle aimed at long-term substantive improvements instead of small optimizations.
*   **Are you working in a high stakes area, like healthcare or autonomous vehicles? What’s the cost of user failure?**
    *   If it’s high, you probably will want to avoid agile development: after all, it’s built in large part around learning from the real failures of real users.
    *   If that’s a user unfairly failing to reach the next level of a game, that’s fine.
    *   If that’s a doctor entering the wrong dosage of a medication into a new interface, that’s not fine.
*   **You’ll also need to think of development costs.**
    *   Agile development relies on being able to get a product up and out the door quickly and change it frequently.
    *   If any part of your design is reliant on the hardware, then agile development presents challenges: it might be easy to roll out a software update to improve a car’s screen interface, but you can’t download a car to fix a hardware problem.
*   In many ways HCI and Agile development are a nice match:
    *   **both emphasize feedback cycles**
    *   **both emphasize getting user feedback**
    *   **both emphasize rapid changes.**



---


**3.8 Conclusion to Methods**

**[GOAL #1] Students will understand the design life cycle as a complete, iterative process for interface design.**



*   **Design life cycle**
    *   We take the results of our initial iteration through the design cycle and use the results to return to the needfinding process.
        *   That’s not to say we need to redo everything from scratch, but our prototypes and evaluation have now increased our understanding of the problem.
        *   There are things we learn by prototyping and evaluating about the task itself.
        *   The evaluation process may have also given us new questions we want to ask users to understand the task better.
    *   In many ways, synthesizing our experiences with the evaluation is our next needfinding process.
    *   We then move on to design alternatives: again, that doesn’t mean starting from scratch and coming up with all new ideas.
        *   Here it means expanding on our current ideas, fleshing them out a bit more, and brainstorming them in terms of those personas and scenarios we used previously.
        *   We might also come up with whole new ideas here.
    *   Then, more prototyping.
        *   At this point, we might discover that as we try to increase the fidelity of our prototypes, the technology or resources aren’t quite there yet.
        *   For example, while the gesture interface might have been promising in the Wizard of Oz prototype, we don’t yet have the technology to recognize gestures that way on the go.
        *   Or we might find that the expense related to the prototype is unfeasible, or the realizing the prototype would require violating some of our other user needs.
        *   For example, we could do gesture recognition if we had users hold a physical device that could recognize gestures, but that might be too expensive to produce, and it might conflict with our audience’s need for a hands-free system.
        *   So we move on with the prototypes that we can build, with the goal of getting to the feedback stage as quickly as possible.
        *   For voice, instead of trying to build a full voice recognition system, maybe we just build a system that can recognize very simplistic voice commands. Instead of recognizing words, maybe it just recognizes the number of utterances if that’s easier to build.
        *   For the screen, maybe we build a wireframe prototype that moves between different screens on a phone, but we don’t connect it to a real system. We still have someone Wizard of Oz it while running along with the participant. That way we focus on usability instead of things that take a lot of work to get right and might end up unnecessary if we find that the prototype isn’t useful.
    *   Then, we evaluate again.
        *   This time, we probably get a little more objective.
        *   We still want data on the qualitative user experience, but we also want data on things like: how long does it take a user to perform the desired actions in the interface? What prevents them from working with the interface?
        *   Imagine that we found, for instance, that for many exercisers, they go through places that are too loud for voice commands to work.
        *   Or, we find that the time it takes to pull out the interface and interact is too distracting.
        *   That information is once again useful to our ongoing iteration.
    *   At the end of that process, we again have some higher-fidelity prototypes, but no product yet. So, we go again.
    *   When you launch the product, instead of having a handful of users we bring in to use our interface, we have hundreds of users using it in ways we never expected
        *   And the cycle begins again.
        *   We have data we’re automatically collected either through usage tracking or error logs. We have user reviews or feedback they submit.
        *   So, we jump back into needfinding using the data we have available to us.
            *   We might find subtle needs or more novel new needs
        *   So the process starts again, this time with live users’ data.
        *   And in general, it never really ends.
            *   Nowadays, you very rarely see interfaces, apps, programs, or web sites that are intentionally put up once and never changed. That might happen because the designers got busy or the company went out of business, but it’s rarely one-off by design.
            *   And as the design evolves over time with real data, you’ll start to see nested feedback cycles: week to week small additions give way to month-to-month updates and year-to-year reinventions. In many ways, your interface becomes like a child: you watch it grow up and take on a life of its own.

**[GOAL #2] Students will understand the relationship between research methods and design principles.**



*   In many ways, design principles capture takeaways and conclusions found by this design life cycle in the past in ways that can be transferred to new tasks.
*   **Map human abilities and task analysis to needfinding**
    *   In uncovering needs, many of our needs are driven by our current understanding of human abilities.
    *   Task analysis allows us to describe those needs, those tasks, in formal ways to equip the interface design process.
*   **Map direct manipulation, mental models, and distributed cognition to design alternatives**
    *   Direct manipulation gives us a family of techniques that we want to emphasize in coming up with our design alternatives.
    *   Mental models provide us an understanding of how the design alternatives might mesh with the user’s understanding of the task.
    *   Distributed cognition gives us a view on interface design that lends itself to design at a larger level of granularity.
*   **Map design principles, representations and invisible interfaces to prototyping**
    *   Design principles give us some great rules of thumb to use when creating our initial prototypes and designs.
    *   Our understanding of representations ensures that the prototypes we create match with users’ mental models.
    *   Invisible interfaces help us remember that the interface should be the conduit between the user and the task, not the focus of attention itself.
*   **Map feedback cycle and interfaces and politics to evaluation**
    *   The vocabulary of the feedback cycle, the gulfs of execution and evaluation, give us ways to evaluate the interfaces that we design.
    *   The notion of politics in interface allow us to evaluate the interface not just in terms of its usable interactions, but in the types of society it creates or preserves.
    *   Those principles of HCI were all found through many years of going through the design life cycle, creating different interfaces, and exploring and evaluating their impact.
*   By leveraging those lessons, we can speed to usable interfaces much faster.
*   **Approaches to user-centered design**
    *   At a minimum, user-centered design advocates involving users throughout the process through surveys, interviews, evaluations, and more that we’ll talk about.
    *   However, user-centered design can be taken to even greater extremes through a number of approaches beyond what we’ve covered.
    *   **Participatory design**
        *   In participatory design, all the stakeholders -- including the users themselves -- are involved as part of the design team.
        *   They aren’t just a source of data, they’re actually members of the design team working on the problem.
        *   That allows the user perspective to be omnipresent throughout the design process.
        *   Of course, there’s still a danger there: generally, we are not our user, but in participatory design one of the designers is the user… but they’re just one user.
        *   So, it’s a great way to get a user’s perspective, but we must also be careful not to over-represent that one user’s view.
    *   **Action research**
        *   Action research is a methodology that addresses an immediate problem, and researches it by trying to simultaneously solve it.
        *   Data gathered on the success of the approach is then used to inform the understanding of the problem and the future approaches.
        *   Most importantly, like participatory design, action research is undertaken by the actual users.
        *   For example, a teacher might engage in action research by trying a new activity in his classroom and reflecting on the results, or a manager might use action research by trying a new evaluation system with her employees and noting the changes.
    *   **Design-based research**
        *   Design-based research is similar to action research, but it can be done by outside practitioners, too.
            *   It’s especially common in learning sciences research.
        *   In design-based research, designers create interventions based on current understanding of the theory and the problem, and use the success of those interventions to improve our understanding of the theory or the problem.
        *   For example, if we believed a certain intersection had a lot of jaywalkers because the signs had poor visibility, we might interview people at the intersection for their thoughts: or, we could create a solution that assumes we’re correct, and then use it to evaluate whether or not we were correct. If we create a more clearly visible sign and it fixes the problem, then it suggests our initial theory was correct.
        *   In all these approaches, notice iteration still plays a strong role: we never try out just one design and stop. We run through the process, create a design, try it out, and then iterate and improve on it. Interface design is never done: it just gets better and better as time goes on, while also adjusting to new trends and technologies.

**EXAM #2 END**
